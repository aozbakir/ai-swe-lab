{
  "metadata": {
    "source_url": "https://ai-act-service-desk.ec.europa.eu/en/faq",
    "access_date": "2025-11-05"
  },
  "results": [
    {
      "question": "What is the AI Act, and what are its objectives?",
      "reference_answer": "The EU AI Act is the world's first comprehensive AI law. It aims to promote innovation and uptake of AI, while ensuring a high level of protection of health, safety and fundamental rights, including democracy and the rule of law. The uptake of AI systems has a strong potential to bring societal benefits, economic growth and enhance EU innovation and global competitiveness. However, in certain cases, the specific characteristics of certain AI systems may create risks related to user safety, including physical safety, and fundamental rights. Some powerful AI models that are widely used could also pose systemic risks. This leads to legal uncertainty and potentially slower uptake of AI technologies by public authorities, businesses and citizens, due to the lack of trust. Disparate regulatory responses by national authorities could risk fragmenting the internal market. Responding to these challenges, legislative action was needed to ensure a well-functioning internal market for AI systems and models where both benefits and risks are adequately addressed.",
      "generated_answer": "The AI Act, as mentioned in the context provided, is titled \"2020/1828 (Artificial Intelligence Act) (OJ L, 2024/1689, 12.7.2024, ELI: http://data.europa.eu/eli/reg/2024/1689/oj).\" This act is aimed at regulating artificial intelligence systems to ensure they are safe and do not pose risks to public health, safety, fundamental rights, or society as a whole.\n\nThe objectives of the AI Act include:\n- Ensuring that AI systems placed on the market or put into service comply with the regulation.\n- Monitoring and supervising compliance of AI systems based on general-purpose AI models developed by the same provider.\n- Encouraging and facilitating the drawing up, review, and adaptation of codes of practice for AI systems, taking into account international approaches. Providers of general-purpose AI models could be invited to participate in this process.\n\nSpecifically, the act aims to:\n1. Ensure that AI systems are safe and do not pose risks.\n2. Monitor compliance with obligations under the regulation.\n3. Encourage the development of adequate technical solutions and appropriate access controls for AI systems.\n4. Promote international approaches in the creation and adaptation of codes of practice.\n\nThese objectives are detailed further through various articles, such as Articles 111, 75, and others mentioned in the context, which discuss market surveillance, mutual assistance, and control measures for general-purpose AI systems.",
      "elapsed_time_sec": 117.85021543502808
    },
    {
      "question": "When does the AI Act go into effect? What is its timeline for implementation?",
      "reference_answer": "The AI Act applies progressively, with a full roll-out by 2 August 2027. The prohibitions, definitions and provisions related to AI literacy became applicable on 2 February 2025; The rules on governance and the obligations for general-purpose AI models became applicable on 2 August 2025; The obligations regarding high-risk systems listed in Annex III, the transparency requirements (Article 50) as well as the measures in support of innovation will apply as of 2 August 2026. This is also the date when the enforcement of AI Act will start; The obligations for high-risk AI systems that classify as high-risk because they are embedded in regulated products, listed in Annex I (list of Union harmonisation legislation), will enter into force on 2 August 2027.",
      "generated_answer": "The Artificial Intelligence Act (2020/1828) goes into effect as of 12 July 2024, as stated in the context provided (\"OJ L, 2024/1689, 12.7.2024\"). \n\nFor implementation:\n- Paragraphs 1 and 2 of Article 111 provide that AI systems which are components of large-scale IT systems established by legal acts listed in Annex X should be placed on the market without prejudice to the application of certain provisions.\n- The timeline for Member States to lay down and notify rules on penalties, including administrative fines, is not explicitly stated but is intended to enable providers to demonstrate compliance on time. Specifically, it mentions \"May 2025\" in view of enabling providers to demonstrate compliance on time.\n\nThe context does not provide a specific date for the full implementation or enforcement timeline beyond these points.",
      "elapsed_time_sec": 71.60163593292236
    },
    {
      "question": "Are revisions to the AI Act expected in the near future?",
      "reference_answer": "The AI Act is designed as a flexible and future-proof regulation that allows to adapt the rules to the rapid pace of technological development, as well as the potential changes in the use of AI systems and emerging risks. While in general the AI Act can only be amended through the legislative procedure, in certain cases, the Commission is empowered to amend certain parts of the AI Act. For instance, the following parts of the AI Act can be adapted by the Commission: The list of high-risk use-cases in Annex III. The Commission is obliged to carry out a yearly review to assess if changes to the list are needed. The threshold above which general-purpose AI models are presumed to have high impact capabilities and are classified as presenting systemic risks. The Commission also regularly assesses if other changes to the AI Act are needed and reports to the European Parliament and the Council. Such regular evaluation is foreseen directly in the AI Act.",
      "generated_answer": "I don't find this information in the provided context.\n\nThe context focuses on specific deadlines for compliance with the Artificial Intelligence Act (2020/1828) and does not mention any plans or expectations regarding revisions to the AI Act. Therefore, based solely on the given context, there is no information that suggests expected revisions to the AI Act in the near future.",
      "elapsed_time_sec": 43.202433824539185
    },
    {
      "question": "What type of systems are regulated under the AI Act?",
      "reference_answer": "The AI Act does not apply to all AI solutions, but only to those that fulfil the definition of an 'AI system' within the meaning of Article 3(1) AI Act. The AI Act follows a risk-based approach and introduces rules for AI systems based on the level of risk they can pose. Any AI practices with an unacceptable risk to health, safety or fundamental rights enshrined in the Charter of Fundamental Rights are prohibited (e.g. AI systems used to detect emotions of employees at work, except medical and safety reasons; certain social scoring practices). AI systems with high risk for health, safety or fundamental rights need to meet certain requirements to make sure they are safe and trustworthy (e.g. AI systems used at border control management; law enforcement, or autonomous vehicles could be examples of high-risk AI systems). Certain AI systems need to meet transparency requirements (e.g., deep fakes will have to be labelled as AI-generated; chatbots should inform that a person is not communicating with a human). All other AI systems remain unregulated and can be placed on the market, put into service or used in the EU without any requirements – at the time of preparation of the proposal of AI Act, it was estimated that these would be 85%.",
      "generated_answer": "The AI Act regulates several types of systems:\n\n1. **AI Systems**: These are machine-based systems designed to operate with varying levels of autonomy, exhibit adaptiveness after deployment, and infer outputs such as predictions, content, recommendations, or decisions that can influence physical or virtual environments.\n\n2. **High-Risk AI Systems**: Specifically mentioned in the context provided, these systems are subject to additional scrutiny and compliance measures due to their potential high-risk nature.\n\n3. **General-Purpose AI Models**: These models are also regulated under certain conditions as specified in the Regulation.\n\n4. **Union Institutions, Bodies, Offices, and Agencies**: When acting as providers or deployers of an AI system, these entities are subject to the requirements of this Regulation.\n\n5. **AI Systems for Military, Defence, or National Security Purposes**: These systems are excluded from the scope of this Regulation if they are placed on the market, put into service, or used with or without modification for such purposes.\n\nSpecific parts of the context that support these answers include:\n- Article 3(1) defines 'AI system'.\n- Paragraphs related to high-risk AI systems and general-purpose AI models.\n- The exclusion clause regarding military, defence, or national security purposes.",
      "elapsed_time_sec": 83.48736906051636
    },
    {
      "question": "Will the AI Act apply to the systems already used on the market?",
      "reference_answer": "The AI Act does not automatically apply to all AI systems placed on the market before its application date. Instead, compliance obligations are phased in depending on the category and whether the system undergoes significant modifications. The AI Act will apply from 31 December 2030 to AI systems which are components of the large-scale IT systems established by the legal acts listed in Annex X that have been placed on the market or put into service before 2 August 2027. In case such large-scale IT systems are evaluated or the legal acts in Annex X are replaced or amended before 31 December 2030, the AI Act shall be taken into account. The AI Act will also apply to high-risk AI systems that have been placed on the market or put into service before 2 August 2026 only in case those systems are significantly modified. The AI Act will also apply from 31 December 2030 to high-risk AI systems that have been placed on the market or put into service before 2 August 2026 and are (intended to be) used by public authorities. Lastly, the AI Act will apply from 2 August 2027 to general-purpose AI models that have been placed on the market before 2 August 2025. However, the rules on prohibited AI practices (Article 5) apply to all AI systems, without taking into account the date of their placement on the market.",
      "generated_answer": "Yes, the AI Act will apply to AI systems already placed on the market or put into service under certain conditions as specified in Article 111 of the provided context.\n\nSpecifically:\n- Without prejudice to the application of Article 5 as referred to in Article 113(3), point (a), AI systems which are components of large-scale IT systems established by legal acts listed in Annex X that have been placed on the market or put into service must comply with this regulation.\n- If a high-risk AI system is modified substantially in a way that it remains a high-risk AI system, or if its intended purpose is changed from military, defense, or national security purposes to other purposes like civilian or humanitarian ones, then it falls within the scope of this Regulation.\n\nThese conditions ensure that even systems already on the market are subject to the regulations when specific criteria are met.",
      "elapsed_time_sec": 70.19906187057495
    },
    {
      "question": "Are \"logic-based\" or \"knowledge-based\" approaches also covered by the AI system definition?",
      "reference_answer": "Focusing specifically on the building phase of the AI system, Recital 12 of the AI Act further clarifies that '[t]he techniques that enable inference while building an AI system include machine learning approaches that learn from data how to achieve certain objectives, and logic- and knowledge-based approaches that infer from encoded knowledge or symbolic representation of the task to be solved.' Those techniques should be understood as 'AI techniques'. In addition to various machine learning approaches, that are often understood as an AI technique, the second category of AI techniques mentioned in Recital 12 of the AI Act are 'logic- and knowledge-based approaches that infer from encoded knowledge or symbolic representation of the task to be solved'. Instead of learning from data, these AI systems learn from knowledge including rules, facts and relationships encoded by human experts. Based on the knowledge encoded by human experts, these systems can 'reason' via deductive or inductive engines or using operations such as sorting, searching, matching and chaining. By using logical inference to draw conclusions, such systems apply formal logic, predefined rules or ontologies to new situations. Logic and knowledge based approaches include, for instance, knowledge representation, inductive (logic) programming, knowledge bases, inference and deductive engines, (symbolic) reasoning, expert systems and search and optimisation methods.",
      "generated_answer": "I don't find this information in the provided context. The context discusses logic- and knowledge-based approaches as part of AI systems but does not explicitly state whether these are included within the definition of AI systems as per the given text.",
      "elapsed_time_sec": 32.99819350242615
    },
    {
      "question": "What distinguishes an AI model from an AI system? Will guidance be issued?",
      "reference_answer": "The AI Act distinguishes between an AI system, defined in Article 3(1) and a general-purpose AI model defined in Article 3(63). The AI Act refers to AI models as those that 'are essential components of AI systems. They do not constitute AI systems on their own. AI models require the addition of further components, such as for example a user interface, to become AI systems. AI models are typically integrated into and form part of AI systems' (Recital 97). 'Large generative AI models are a typical example for a general-purpose AI model, given that they allow for flexible generation of content, such as in the form of text, audio, images or video, that can readily accommodate a wide range of distinctive tasks' (Recital 99). The AI Act defines an AI system as a 'machine-based system that is designed to operate with varying levels of autonomy and that may exhibit adaptiveness after deployment, and that, for explicit or implicit objectives, infers, from the input it receives, how to generate outputs such as predictions, content, recommendations, or decisions that can influence physical or virtual environments' (Article 3(1), Recital 12). For more information, please refer to the Guidelines on the definition of an artificial intelligence system.",
      "generated_answer": "An AI model is distinguished from an AI system by the fact that it does not constitute an AI system on its own. According to the context provided:\n\n- \"AI models are essential components of AI systems.\"\n- \"They do not constitute AI systems on their own.\"\n- \"AI models require the addition of further components, such as a user interface, to become AI systems.\"\n\nRegarding guidance, the context mentions that the Commission is expected to carry out appropriate consultations during its preparatory work. Specifically, it states:\n\n- \"It is of particular importance that the Commission carry out appropriate consultations during its preparatory work, including at expert level.\"\n- \"Those consultations be conducted in accordance with the principles laid down in the Interinstitutional Agreement of 13 April 2016 on Better Law-Making (55).\"\n\nWhile this does not explicitly state that guidance will be issued, it implies that such actions are part of the preparatory process. Therefore, based on the provided context, we can infer that guidance is likely to be developed as a result of these consultations.\n\nReference: Parts cited from the context directly support these answers.",
      "elapsed_time_sec": 81.2709448337555
    },
    {
      "question": "What does the 'risk-based approach' mean?",
      "reference_answer": "The AI Act follows a risk-based approach, classifying AI systems into four different risk categories: unacceptable risk, high risk, transparency risk, and minimal to no risk. For the unacceptable risk category, the AI Act lists specific practices that are prohibited (Article 5 of the AI Act). The high-risk AI systems are defined in accordance with Article 6 of the AI Act in conjunction with Annex I (list of Union harmonisation legislation) and Annex III of the AI Act. Annex III comprises eight areas in which the use of AI can be particularly sensitive and lists concrete use cases for each area that are assessed by the co-legislator as posing significant risks to health, safety and fundamental rights. For certain AI systems where transparency is particularly important, rules on transparency are foreseen. All the other AI systems are considered as \"minimal to no risk\" AI systems and the AI act does not foresee any obligations for them. Voluntarily, providers of those systems may choose to apply the requirements for trustworthy AI and adhere to voluntary codes of conduct.",
      "generated_answer": "I don't find this information in the provided context.",
      "elapsed_time_sec": 24.751655340194702
    },
    {
      "question": "What are the obligations for high-risk, transparency-risk, and minimal-risk AI systems?",
      "reference_answer": "Depending on the level of risk, obligations and requirements related to specific AI systems will vary. Providers of high-risk AI systems need to ensure that their high-risk AI system complies with the requirements of the AI Act before the system is placed on the market or put into service. Examples of such requirements include having a risk management system in place, recording logs, and ensuring qualitative data and data governance, as well as human oversight. In addition, providers will need to register their system in the EU database and accompany the system with instructions for its use for deployers. Providers of transparency-risk systems, such as chatbots, AI social companions or deep fakes, need to comply with certain transparency obligations. This includes, for example, informing natural persons that they are interacting with an AI system or ensuring that AI generated content can be detected as such. Deployers of systems that can generate or manipulate content need to disclose that the content is generated by AI. The AI Act does not prescribe obligations for minimal-risk AI systems, but Member States can facilitate the drawing up of voluntary codes of conduct for AI systems that are not high-risk.",
      "generated_answer": "Based on the provided context, here are the obligations for different categories of AI systems:\n\n1. **High-Risk AI Systems**: \n   - They should be designed in a manner that enables deployers to understand how the AI system works.\n   - Deployers must evaluate its functionality and comprehend its strengths and limitations.\n   - Transparency is required before these systems are placed on the market or put into service.\n   - Providers of such systems should cooperate closely with providers of relevant high-risk AI systems to ensure compliance.\n\n2. **Transparency-Risk AI Systems**:\n   - The context does not explicitly mention specific obligations for transparency-risk AI systems. However, it implies that these systems should also have certain transparency requirements similar to those for high-risk AI systems.\n   - Natural persons interacting with these systems must be notified unless the interaction is obvious.\n\n3. **Minimal-Risk AI Systems**:\n   - The context does not provide specific obligations for minimal-risk AI systems. Minimal-risk systems are generally not subject to the same stringent regulations as high-risk or transparency-risk systems, but they may still have some general compliance requirements depending on their use and functionality.\n\nFor detailed information about minimal-risk AI systems, one would need additional regulatory text beyond what is provided in this context.",
      "elapsed_time_sec": 92.62480854988098
    },
    {
      "question": "How should evolving AI systems be handled?",
      "reference_answer": "In principle, the evolution of an AI system does not have legal implications. However, in the case of high-risk AI systems, such developments should be anticipated and addressed within the risk management framework and related compliance obligations. Where an AI system undergoes substantial modifications, this may give rise to legal consequences, including the need for a renewed conformity assessment under the AI Act.",
      "generated_answer": "Evolving AI systems, particularly those that continue to learn after being placed on the market or put into service, should be managed by ensuring that changes which are pre-determined by the provider and assessed at the moment of the initial conformity assessment do not constitute a substantial modification. These predetermined changes and their performance should be included in the technical documentation referred to in point 2(f) of Annex IV.\n\nThis approach allows for oversight while accommodating the dynamic nature of AI systems that adapt over time. Natural persons can oversee the functioning of these systems, ensure they are used as intended, and address any impacts throughout the system’s lifecycle.",
      "elapsed_time_sec": 55.17402124404907
    },
    {
      "question": "Is an \"AI Officer\" or governance board recommended within companies?",
      "reference_answer": "The AI Act does not require any particular internal governance within the company. However in line with Article 17(1)(m) the providers of high-risk AI systems should put a quality management system in place, that should include an accountability framework, setting up the responsibilities of the management and other staff with regard to all the aspects related to the quality management system listed in Article 17 of the AI Act.",
      "generated_answer": "I don't find this information in the provided context. The context discusses governance at the Union level, specifically mentioning the AI Office and the European Artificial Intelligence Board. It does not provide any recommendations regarding AI officers or governance boards within companies.",
      "elapsed_time_sec": 31.239641904830933
    },
    {
      "question": "How do I access AI-related funding programs?",
      "reference_answer": "The EU Funding & Tenders Portal of the European Commission is the central place to find any funding opportunities from the EU, including AI-relevant ones. Of particular interest among the funding programmes are Horizon Europe and Digital Europe Programme. Within Horizon Europe, there are different areas of interest, namely Cluster 4 and the European Innovation Council. Other areas such as the bottom-up European Research Council may be also relevant. The GenAI4EU flagship initiative offers ample opportunities to develop and deploy trustworthy, generative AI in Europe's strategic sectors. With nearly 700 million euros committed, there are plenty of opportunities to help Europe become more competitive and innovative. A dedicated website gives an overview of the different call for proposals available under Horizon Europe and the Digital Europe Programme.",
      "generated_answer": "I don't find this information in the provided context.",
      "elapsed_time_sec": 22.103679418563843
    },
    {
      "question": "What is AI literacy as described in Article 4 of the AI Act?",
      "reference_answer": "The concept of AI literacy mentioned in Article 4 of the AI Act relies on the definition of the term given in Article 3(56) of the AI Act, according to which: 'AI literacy' means skills, knowledge and understanding that allow providers, deployers and affected persons, taking into account their respective rights and obligations in the context of this Regulation, to make an informed deployment of AI systems, as well as to gain awareness about the opportunities and risks of AI and possible harm it can cause. Read the complete AI Literacy FAQ here.",
      "generated_answer": "AI literacy, as described in Article 4 of the AI Act, refers to skills, knowledge, and understanding that allow providers, deployers, and affected persons to make an informed deployment of AI systems. This includes awareness about the opportunities and risks associated with AI and the potential harm it can cause. The necessary notions may vary depending on the rights and obligations of the individuals involved in the context of this Regulation.\n\nReference: Article 4 (56)",
      "elapsed_time_sec": 42.64814019203186
    },
    {
      "question": "How can companies ensure AI competency (e.g., employee training)?",
      "reference_answer": "There is no single approach that works for all. It depends on the type of AI system that the company deals with and the knowledge of the staff in question. There is, however, no obligation for external training or external certification. Please see further AI Literacy - Questions & Answers.",
      "generated_answer": "I don't find this information in the provided context. The context discusses various aspects related to high-risk AI systems, including oversight during testing, reversibility of decisions, and data governance. However, it does not specifically address how companies can ensure AI competency through employee training or other means.",
      "elapsed_time_sec": 30.66109800338745
    },
    {
      "question": "Will the Commission provide training materials or courses for AI Officers?",
      "reference_answer": "The AI Act does not require that companies designate AI Officers. However, the AI Act requires that providers and deployers of AI systems ensure, to their best extent, a sufficient level of AI literacy of their staff and other people dealing with the operation and use of AI systems on their behalf. The Commission published a living repository to foster learning and exchange on AI literacy, Questions and Answers on AI literacy, and a recorded webinar on AI literacy. The repository will be expanded. There are many cost-free online training courses. The choice depends on the individual skills of the staff in question.",
      "generated_answer": "I don't find this information in the provided context. The context discusses promoting AI literacy and drawing up voluntary codes of conduct but does not specifically mention providing training materials or courses for AI Officers.",
      "elapsed_time_sec": 26.557708978652954
    },
    {
      "question": "What AI systems are prohibited?",
      "reference_answer": "The prohibitions of Article 5 AI Act include certain manipulative practices, social scoring, predictive policing based solely on profiling, scraping the internet and CCTV material to build-up or expand facial recognition databases, emotion recognition at education and workplace, biometric categorisation to infer sensitive attributes such as political opinion or sexual orientation and real-time remote biometric identification for law enforcement purposes at publicly accessible spaces (with some limited exceptions). For more information, please refer to the Guidelines on prohibited artificial intelligence practices.",
      "generated_answer": "The prohibited AI systems include those that deploy subliminal techniques beyond a person’s consciousness or purposefully manipulative or deceptive techniques, which have the objective or effect of materially distorting the behavior of a person or group of persons by appreciably impairing their ability to make an informed decision. This prohibition is laid down in Article 5(a) of the Regulation.\n\nAdditionally, AI systems that are specifically developed and put into service for scientific research and development should not be prohibited if they comply with recognized ethical and professional standards. However, any use or practice violating data protection law, non-discrimination law, consumer protection law, and competition law is also prohibited by Union law and thus not covered under the exceptions provided in this Regulation.\n\nI don't find this information in the provided context.",
      "elapsed_time_sec": 58.694146156311035
    },
    {
      "question": "What systems are prohibited under Article 5 of the AI Act (e.g., social scoring, emotion recognition)?",
      "reference_answer": "The prohibitions of Article 5 include certain manipulative practices, social scoring, predictive policing based solely on profiling, scraping the internet and CCTV material to build-up or expand facial recognition databases, emotion recognition at education and workplace, biometric categorisation to infer sensitive attributes such as political opinion or sexual orientation and real-time remote biometric identification for law enforcement purposes at publicly accessible spaces (with some limited exceptions).",
      "generated_answer": "Under Article 5 of the AI Act, the following systems are prohibited:\n\n1. **AI Systems Deploying Subliminal Techniques or Manipulative/Deceptive Techniques**: These systems are prohibited if they place on the market, put into service, or use such techniques beyond a person’s consciousness with the objective or effect of materially distorting a person's behavior by appreciably impairing their ability to make decisions. This can lead to outcomes that violate rights to dignity and non-discrimination as well as values of equality and justice.\n\n2. **Social Scoring Systems**: These systems evaluate or classify natural persons or groups based on multiple data points related to social behavior in various contexts, known inferred, or predicted personal characteristics over certain periods of time. The social score obtained from such AI systems may lead to the exclusion of certain groups and violate rights and values mentioned above.\n\n3. **Emotion Recognition Systems**: These are also prohibited unless they are not otherwise prohibited under this Regulation.\n\nThese prohibitions aim to protect individuals' rights, dignity, non-discrimination, equality, and justice by preventing AI systems from engaging in practices that can manipulate or distort behavior in harmful ways.",
      "elapsed_time_sec": 85.75693440437317
    },
    {
      "question": "Will guidelines be issued for prohibited AI systems?",
      "reference_answer": "Guidelines were issued in February 2025.",
      "generated_answer": "Based on the provided context, there is no direct mention of guidelines being issued specifically for prohibited AI systems. The context discusses the scope of Regulation (EU) 2016/679 regarding AI systems and mentions that this Regulation does not apply to certain activities like research, testing, or development prior to placing AI systems on the market or putting them into service. It also notes that the Commission shall submit appropriate proposals to amend the Regulation if necessary, considering various factors.\n\nHowever, it does state that \"To guide the evaluations and reviews referred to in paragraphs 1 to 7 of this Article, the AI Office shall undertake to...\" This suggests that guidelines might be developed by the AI Office for evaluating and reviewing AI systems, but it is not explicitly stated whether these would include prohibited AI systems.\n\nGiven the information provided, a precise answer regarding the issuance of specific guidelines for prohibited AI systems cannot be confirmed. Therefore:\n\nI don't find this information in the provided context.",
      "elapsed_time_sec": 82.3140881061554
    },
    {
      "question": "When will some guidelines on the classification of high-risk AI systems be published?",
      "reference_answer": "The Commission is currently preparing guidelines on the classification of high-risk AI systems, which should be published in February 2026.",
      "generated_answer": "The guidelines on the classification of high-risk AI systems are scheduled to be published no later than 2 February 2026. This is based on the statement in point 5 of the provided context, which says, \"The Commission shall, after consulting the European Artificial Intelligence Board (the ‘Board’), and no later than 2 February 2026, provide guidelines specifying the practical implementation of this Article in line with Article 96 together with a comprehensive list of practical examples of use cases of AI systems that are high-risk and not high-risk.\"",
      "elapsed_time_sec": 50.91137385368347
    },
    {
      "question": "Is emotion recognition prohibited in workplace/education settings or for these purposes (e.g. employee monitoring)?",
      "reference_answer": "The placing on the market, the putting into service for this specific purpose, or the use of AI systems to infer emotions of a natural person in the areas of workplace and education institutions, is prohibited. However, this prohibition does not apply to the use of an AI system when the AI system is intended to be put in place or into the market for medical or safety reasons.",
      "generated_answer": "I don't find this information in the provided context.\n\nThe context does not explicitly state that emotion recognition is prohibited in workplace or education settings. Instead, it emphasizes the need to inform natural persons about the operation of such systems and process personal data according to specific regulations. The text also highlights concerns regarding the intrusive nature of these systems in work or educational contexts but does not indicate a prohibition on their use for these purposes.",
      "elapsed_time_sec": 42.710081815719604
    },
    {
      "question": "What is the difference between the list of use cases and the list of areas of Annex III?",
      "reference_answer": "Annex III comprises eight areas and lists specific use-cases for each, which the EU co-legislator has assessed as posing a high risk of harm to health and safety or to fundamental rights. In simple terms, the areas are enumerated as points in Annex III (e.g. point 1 – Biometrics, point 2 – Critical infrastructure, point 3 – Education and vocational training, etc.). The use-cases are listed under each area. For example, point 6 of Annex III covers the area of law enforcement, which includes five use-cases ((a)–(e)). To be classified as high-risk, an AI system must fall under one of the areas of Annex III and correspond to one of the use-cases listed therein. In other words, not all AI systems used in a particular area (such as law enforcement) are automatically considered high-risk; only those falling within the specific use-cases identified in Annex III qualify.",
      "generated_answer": "I don't find this information in the provided context.",
      "elapsed_time_sec": 24.643595218658447
    },
    {
      "question": "What documentation/assessments are needed for providers of high-risk AI systems to comply with the AI Act?",
      "reference_answer": "Article 16 of the AI Act gives an overview of the obligations that providers of high-risk AI systems need to follow in order to comply with the AI Act. Prior to placing their high-risk AI system or putting it into services, providers of such systems need to ensure that their high-risk AI system is compliant with Articles 8-15 of the AI Act and has undergone a conformity assessment. This conformity should be demonstrated upon a reasonable request by a competent national authority. Following this conformity assessment, providers need to draw up an EU declaration of conformity and affix the CE marking to the system (or on its packaging or accompanying documentation), as well as indicate their contact information on the high-risk AI system so they can be contacted. Providers also need to register their high-risk AI system in the EU database. In addition, providers need to comply with Articles 17-20 AI Act by setting up a quality management system, keep documentation for 10 years, keep automatically generated logs by the high-risk AI system, and take necessary corrective actions and provide information related to these actions. Finally, providers of high-risk AI systems need to ensure that their system complies with relevant accessibility requirements.",
      "generated_answer": "Providers of high-risk AI systems need to ensure compliance with the requirements set out in Section 2 of this Chapter and Annex VII, specifically points 4.3., 4.4., 4.5., and the fifth paragraph of point 4.6. The necessary testing and reporting processes, information, and documentation should be integrated into existing documentation and procedures required under Union harmonisation as set out in Article 49(2).\n\nAdditionally, upon request from national competent authorities, providers must provide documentation of the assessment. High-risk AI systems should include general characteristics, capabilities, limitations, algorithms, data, training, testing, and validation processes used, along with documentation on the relevant risk-management system.\n\nThe technical documentation should be kept up to date throughout the lifetime of the AI system. Notified bodies that have been notified under the legal acts mentioned can control the conformity of high-risk AI systems with these requirements, provided their compliance includes all specified elements.\n\nFurthermore, testing in real-world conditions outside AI regulatory sandboxes may be conducted by providers or prospective providers listed in Annex III, following a real-world testing plan. The Commission will specify the detailed elements of this plan through implementing acts.\n\nReference: Section 2 of this Chapter and Annex VII, points 4.3., 4.4., 4.5., and the fifth paragraph of point 4.6; Article 49(2); and the real-world testing plan as per the Commission's implementing acts.",
      "elapsed_time_sec": 88.36766505241394
    },
    {
      "question": "Does location data qualify as biometric data under the AI Act?",
      "reference_answer": "Location data is normally not biometric. Accordingly, it does not fall within the high-risk use cases listed in point 1 of Annex III, nor within the prohibited practices set out in Article 5(1)(g).",
      "generated_answer": "I don't find this information in the provided context.\n\nThe given context focuses on the processing of biometric data, including remote biometric identification systems, but it does not explicitly define what constitutes biometric data or whether location data is included. Therefore, based solely on the provided text, I cannot determine if location data qualifies as biometric data under the AI Act.",
      "elapsed_time_sec": 34.47074580192566
    },
    {
      "question": "Are biometric systems (e.g., facial recognition for bank security) permitted?",
      "reference_answer": "Most biometric AI systems are not prohibited under the AI Act. The prohibited AI practices regarding biometric systems are limited and include in particular: emotion recognition in the workplace, certain biometric categorisation systems, and real-time remote biometric identification in publicly accessible spaces for law enforcement purposes. These are subject to narrowly defined exceptions. There are, however, some additional rules that apply to permitted biometric systems. In particular, certain biometric applications are considered high-risk (remote biometric identification systems, certain AI systems intended for biometric categorisation, and AI systems intended for emotion recognition). The AI Act sets out specific requirements for high-risk AI systems. These requirements relate to, among others, data and data governance, documentation and record-keeping, transparency and the provision of information to users, human oversight, robustness, accuracy, and security. The use of AI systems for biometric verification - that is, confirming that an individual is who they claim to be - is not prohibited under the AI Act and does not fall within the category of high-risk systems. For more information, please refer to Guidelines on prohibited artificial intelligence practices.",
      "generated_answer": "According to the context provided, biometric systems like facial recognition can be permitted under certain conditions. Specifically:\n\n- The use of biometric identification systems within a Member State's territory is only permissible if that Member State has explicitly authorized such use in its detailed rules (see point 37).\n- Critical-use cases of biometric systems are allowed as long as they comply with relevant Union and national law (see the part about \"critical-use cases\").\n- Remote biometric identification systems should not be used for real-time identification in publicly accessible spaces for law enforcement purposes unless strictly necessary for specific objectives (see point h).\n\nTherefore, while facial recognition or other biometric systems can be permitted, their use must comply with these conditions and any relevant laws set by the Member State.",
      "elapsed_time_sec": 52.584107875823975
    },
    {
      "question": "What does the requirement for human oversight mean?",
      "reference_answer": "The EU AI Act emphasises the importance of human oversight for high-risk AI systems. This means that these systems must be designed and developed so that humans can monitor them while they are in use. Appropriate tools and measures should be in place to help people supervise AI effectively, ensuring that any risks to health, safety, or fundamental rights are prevented or minimised. When the system is in use, human oversight also includes appropriate measures to be implemented by the deployer. Those measures should be described in the instructions of use and be effective and commensurate with the risk, the level of autonomy of the system and the context of use of the high-risk AI system. Deployers should implement human oversight measures and assign this task to natural persons who have the necessary competence, training and authority, as well as the necessary support.",
      "generated_answer": "The requirement for human oversight means that high-risk AI systems must be designed and developed so they can be effectively monitored by natural persons during use. This ensures that risks to health, safety, or fundamental rights are prevented or minimized when the system is used as intended or under reasonably foreseeable conditions. Deployers should assign this oversight to competent, trained, and authorized natural persons who have the necessary support. Additionally, the AI systems should be subject to operational constraints that cannot be overridden by the system itself and must be responsive to human operators. This ensures that humans can appropriately control and oversee the AI systems, maintaining technical robustness and safety.\n\nReference: Article 14 paragraphs 1 and 2 of the provided context.",
      "elapsed_time_sec": 50.121439695358276
    },
    {
      "question": "Are there simplified compliance paths for SME providers of high-risk AI systems?",
      "reference_answer": "The AI Act introduces a number of support measures tailored to small and medium-sized enterprises (SMEs). These include the possibility to prepare simplified technical documentation and to implement simplified quality management systems for high-risk AI systems. SMEs are also granted free-of-charge access to AI regulatory sandboxes, enabling them to test and develop AI systems in a controlled environment. In addition, the European Commission and national authorities are required under Article 62 to provide specific support measures to SMEs. Their interests are further represented through a special membership category in the advisory forum, ensuring that their needs are taken into account in the regulatory process. Finally, SMEs benefit from special consideration when penalties are imposed, reflecting their particular position in the market and the potential impact of sanctions.",
      "generated_answer": "I don't find this information in the provided context.\n\nThe context does not mention any specific simplified compliance paths for SME providers of high-risk AI systems. It focuses on general requirements and provisions related to high-risk AI systems, including cooperation between providers and operators, compliance with certain requirements, and support measures like guidance services for prospective providers such as SMEs and start-ups.",
      "elapsed_time_sec": 35.86958980560303
    },
    {
      "question": "What is a general-purpose AI (GPAI) system?",
      "reference_answer": "When a general-purpose AI model is integrated into or forms part of an AI system, the resulting system should be considered a general-purpose AI system if it gains the capability to serve multiple purposes through this integration. General-purpose AI systems can be used directly or embedded in other AI systems. Given their versatility, general-purpose AI systems may be used as high-risk AI systems in their own right or as components of other high-risk AI systems. To ensure a fair distribution of responsibilities throughout the AI value chain, providers of general-purpose AI systems should cooperate closely with providers of relevant high-risk AI systems, unless otherwise specified in this Regulation. This cooperation should enable high-risk AI system providers to comply with their obligations under this Regulation and facilitate interactions with the competent authorities established by the Regulation.",
      "generated_answer": "A general-purpose AI system is an AI system that is based on a general-purpose AI model. This model is trained using large amounts of data through self-supervision at scale, demonstrating significant generality and the capability to competently perform a wide range of distinct tasks. The system can serve various purposes both directly and by being integrated into other AI systems. It is designed to be versatile and adaptable for use in different downstream applications or systems (see context points 63 and 67).",
      "elapsed_time_sec": 44.467254877090454
    },
    {
      "question": "How do AI agents fit within the GPAI framework under the AI Act?",
      "reference_answer": "While AI agents are not a separate category under the AI Act, they may have to comply with the requirements for AI systems and/or the obligations for providers of general-purpose AI models. Particularly relevant are the AI Act's prohibitions of harmful manipulation or exploitation of vulnerabilities, which can require technical safeguards in the design of an agentic AI system. If the agentic AI system classifies as high-risk, it is subject to additional requirements that ensure its safety and trustworthiness. Furthermore, if the agentic AI system is intended to interact with natural persons or generate content, transparency rules apply (for further information, see these FAQs and this consultation). As regards general-purpose AI models, factors like the level of autonomy or tool use of a model can be decisive in its designation as having systemic risk. Providers of general-purpose AI models with systemic risk are subject to risk management obligations, which include considerations regarding the model's autonomous capabilities and its agentic use. Given that the developments are recent and fast evolving, the European Commission's regulatory considerations are only preliminary at this stage, also in light of the fact that the understanding of what constitutes an AI agent varies greatly. The European Commission continues to closely monitor the development of AI agents and, if need be, will consider actions in that regard. For example, the AI Office's recent call for tenders on technical assistance for AI safety includes a dedicated lot to evaluating safety and security of AI agents.",
      "generated_answer": "I don't find this information in the provided context. The given text discusses the role of the Commission, the AI Office, and market surveillance authorities regarding the monitoring and regulation of general-purpose AI models. It mentions codes of practice, transparency requirements, and consultations but does not specifically address how AI agents fit within the GPAI framework under the AI Act.",
      "elapsed_time_sec": 39.22266960144043
    },
    {
      "question": "Which AI models does the AI Act apply to?",
      "reference_answer": "The AI Act regulates amongst others general-purpose AI models that are placed on the Union market, including their development and use. For further information under which conditions an AI model is considered 'general-purpose', see the question 'When does a model qualify as a general-purpose AI model?' and Section 2.1 of the Guidelines on the scope of the obligations for general-purpose AI models. 'Placing on the market' means the first supply of an AI system or a general-purpose AI model for distribution or use on the Union market during a commercial activity, whether in return for payment or free of charge (Article 3(9)(10) AI Act). Some examples of what constitutes a placing on the market can be found in Section 3.1.2 of the Guidelines on the scope of the obligations for general-purpose AI models. An open-source release can constitute a placing on the market. Notably, the commercial supply of the model can be free of charge (Article 3(10) AI Act). However, providers of an open-source general-purpose AI model that does not present systemic risk are exempt from certain obligations. For further information, see the question 'How does the AI Act apply to general-purpose AI models released as open-source?' and Section 4 of the Guidelines on the scope of the obligations for general-purpose AI models. The internal use of a general-purpose AI model constitutes a placing on the market at least if it is essential for providing a product or service to third parties on the Union market or if it affects the rights of natural persons in the Union. Further examples are provided in Section 3.1.2 of the Guidelines on the scope of the obligations for general-purpose AI models. AI models that are specifically developed and put into service for the sole purpose of scientific research and development are out of scope of the AI Act (Article 2(6) AI Act).",
      "generated_answer": "The AI Act applies to general-purpose AI models that are already placed on the market or put into service as of a certain date. Specifically:\n\n- These models should be described with details such as tasks they are intended for, acceptable use policies, release dates and distribution methods, architecture and parameters, modality of inputs and outputs, and licensing information.\n- The Commission should ensure effective implementation through monitoring activities and investigations.\n\nReference: Article 111(2) of the AI Act.",
      "elapsed_time_sec": 47.37533259391785
    },
    {
      "question": "When does a model qualify as a general-purpose AI model?",
      "reference_answer": "Article 3(63) AI Act defines a 'general-purpose AI model' as 'an AI model, including where such an AI model is trained with a large amount of data using self-supervision at scale, that displays significant generality and is capable of competently performing a wide range of distinct tasks regardless of the way the model is placed on the market and that can be integrated into a variety of downstream systems or applications, except AI models that are used for research, development or prototyping activities before they are placed on the market'. This definition lists in a general manner factors that determine whether a model is a general-purpose AI model. Nevertheless, it does not set out specific criteria that potential providers can use to assess whether their model is a general-purpose AI model. Section 2.1 of the Guidelines on the scope of obligations for general-purpose AI models provide an indicative criterion: a model is considered a general-purpose AI model if the computational resources used for its training exceeds 10^23 floating point operations and it can generate language (text or audio), text-to-image, or text-to-video. This compute threshold corresponds to the typical amount of compute used to train models with one billion parameters on large datasets. Moreover, the chosen modalities enable flexible content generation capable of accommodating a wide range of distinct tasks. However, this is not an absolute rule — models meeting this criterion may exceptionally not qualify as general-purpose AI models if they lack significant generality, while models below this threshold may still be general-purpose AI models if they display significant generality and can competently perform a wide range of tasks.",
      "generated_answer": "A model qualifies as a general-purpose AI model if it meets the following criteria according to the provided context:\n\n1. It displays significant generality.\n2. It is capable of competently performing a wide range of distinct tasks regardless of how it is placed on the market.\n3. It can be integrated into a variety of downstream systems or applications.\n\nThese models are typically trained on large amounts of data using various methods such as self-supervised, unsupervised, or reinforcement learning. The context also specifies that general-purpose AI models do not constitute AI systems on their own but require additional components like user interfaces to become AI systems.",
      "elapsed_time_sec": 50.56200075149536
    },
    {
      "question": "When is a general-purpose AI model one with systemic risk?",
      "reference_answer": "A general-purpose AI model is classified as a general-purpose AI model with systemic risk if it meets either of the following two conditions. First, if it has capabilities that match or exceed those of the most advanced models. The AI Act presumes models trained with a cumulative amount of computational resources that exceeds 10^25 floating point operations to have such capabilities (Article 51(2) AI Act). The threshold is set on the basis that the use of this amount of compute indicates that the model could significantly impact the Union market due to its reach or potential negative effects on public health, safety, security, fundamental rights, or society. The European Commission shall adjust this threshold when necessary to account for technological developments. Second, the European Commission can designate a model as a general-purpose AI model with systemic risk, either through its own initiative or following a qualified alert from the scientific panel, if the model has capabilities or an impact equivalent to those of the most advanced models (Article 51(1)(b) AI Act). This accounts for the fact that there are models that may present systemic risks despite not meeting the compute threshold. For a model meeting either condition, its provider must comply with additional obligations, including assessing and mitigating systemic risks (see the question 'What are the additional requirements for general-purpose AI models with systemic risk?'). Providers of models trained with less than 10^25 floating point operations of cumulative compute are not expected to notify the European Commission of their model and comply with the safety and security obligations, unless their model was designated as presenting systemic risks by the European Commission.",
      "generated_answer": "A general-purpose AI model is considered one with systemic risk if it has high-impact capabilities, evaluated on the basis of its specific characteristics. These models could pose systemic risks which include, but are not limited to, any actual or reasonably foreseeable negative effects in relation to major accidents, disruptions of critical sectors and serious consequences to public health and safety; any actual or reasonably foreseeable negative effects on democratic processes, public and economic security; and the dissemination of illegal, false, or discriminatory content. \n\nSpecifically, a general-purpose AI model should be considered to present systemic risks if it has high-impact capabilities, as stated in point (110) of the provided context.",
      "elapsed_time_sec": 54.30219531059265
    },
    {
      "question": "When should providers assess whether their model will be a general-purpose AI model and/or a general-purpose AI model with systemic risk?",
      "reference_answer": "Training a general-purpose AI model (with or without systemic risk) requires considerable upfront allocation of compute resources. Therefore, providers are able to know whether their model will meet the indicative criterion for constituting a general-purpose AI model, given in Section 2.1 of the Guidelines on the scope of the obligations for general-purpose AI models, or the cumulative training compute threshold in Article 51(2) of the AI Act, before the training is complete (recital 112 AI Act). In particular, providers are expected to estimate whether their model will be a general-purpose AI model (trained with more than 10^23 floating point operations of cumulative compute) and a general-purpose AI model with systemic risk (trained with more than 10^25 floating point operations of cumulative compute) before the start of a large pre-training run. Once a model exceeds the cumulative training compute threshold of 10^25 floating point operations, set in Article 51(2) of the AI Act, or the provider knows that the model will exceed this threshold, the provider has two weeks to notify the European Commission (Article 52 AI Act). For further information on the notification see the question 'If a general-purpose AI model is above the compute threshold for such models with systemic risk, is it always considered to have systemic risk?'.",
      "generated_answer": "Providers should assess whether their model will be a general-purpose AI model and/or a general-purpose AI model with systemic risk prior to its first placing on the market. This includes performing necessary model evaluations, which may involve conducting and documenting adversarial testing of models, either internally or through independent external testing. Continuous assessment and mitigation of systemic risks should also be conducted by providers of such models.\n\nReference: \"(112) It is also necessary to clarify a procedure for the classification of a general-purpose AI model with systemic risks.\"",
      "elapsed_time_sec": 49.15407681465149
    },
    {
      "question": "What is the methodology for the cumulative training compute estimation (including synthetic data)?",
      "reference_answer": "Providers may choose their method to calculate/estimate the cumulative training compute in floating point operations. The Guidelines on the scope of the obligations for general-purpose AI models, in their Annex, offer two possible approaches: architecture-based and hardware-based; as well as general principles for what should be included/excluded in the calculation. The compute used to generate synthetic data — artificially created training data produced by algorithms (including other models) rather than collected from real-world sources — must be included in the cumulative training compute calculation, additionally to the compute used to train on the synthetic data, if the synthetic data is not publicly accessible. However, the cumulative training compute of the parent model (i.e., the model generating the synthetic data) does not need to be included in the calculation/estimation.",
      "generated_answer": "I don't find this information in the provided context. The context mentions that the amount of computation used for training the model can be measured in floating point operations or indicated by other variables such as estimated cost, time required for training, or energy consumption. However, it does not provide specific details on how to estimate cumulative training compute, especially regarding synthetic data.",
      "elapsed_time_sec": 37.97387886047363
    },
    {
      "question": "If a general-purpose AI model is above the compute threshold for such models with systemic risk, is it always considered to have systemic risk?",
      "reference_answer": "Providers may contest the automatic classification as a general-purpose AI model with systemic risk, when meeting the compute threshold in Article 51(2) of the AI Act. For instance, they may rebut the presumption that their model has high-impact capabilities by providing evidence that their model's capabilities do not match or exceed those recorded in the most advanced models. Section 2.3 of the Guidelines on the scope of the obligations for general-purpose AI models provides further guidance on how to do this. If the European Commission accepts the provider's arguments, the model will cease to be classified as systemic risk, thereby reducing the obligations for the provider with regard to this specific model from the moment of acceptance onwards. In line with the collaborative and proportionate application of the AI Act, and the provider's right to good administration, the European Commission may take into account new information submitted by the provider after the notification, once it becomes available. Furthermore, the European Commission's decision to accept or reject the provider's arguments may take into account uncertainties in the information provided and outline conditions for potential future reassessment accordingly. However, the obligations for providers of general-purpose AI models with systemic risk apply from the moment the model meets the condition of Article 51(1)(a) of the AI Act (i.e., high-impact capabilities, presumed by meeting the compute threshold).",
      "generated_answer": "I don't find this information in the provided context.\n\nThe context states that \"A general-purpose AI model that meets the applicable threshold for high-impact capabilities should be presumed to be a general-purpose AI model with systemic risk.\" However, it does not explicitly state that such models are always considered to have systemic risk. The term \"presumed\" suggests that while there is a strong indication or expectation of systemic risk based on meeting the compute threshold, this presumption can be reassessed and potentially challenged by the provider presenting sufficiently substantiated arguments.",
      "elapsed_time_sec": 49.19619011878967
    },
    {
      "question": "Do built-in safety measures, such as output filters, affect the classification as a general-purpose AI model with systemic risk?",
      "reference_answer": "Generally, no. Safety measures may be relied upon to mitigate systemic risks stemming from the provider's general-purpose AI model with systemic risk. However, mitigations do not change the fact that the model presents (acceptable) systemic risk. If the model does not present systemic risks, even without implemented safety and security measures, providers may contest the automatic classification pursuant to Article 52(2) of the AI Act (see the question 'If a general-purpose AI model is above the compute threshold for such models with systemic risk, is it always considered to have systemic risk?').",
      "generated_answer": "I don't find this information in the provided context. The context focuses on the classification and risks associated with general-purpose AI models but does not specifically mention built-in safety measures like output filters and their impact on the classification of such models as having systemic risks.",
      "elapsed_time_sec": 34.41558814048767
    },
    {
      "question": "Is every large release of an AI model also a new model in the sense of the AI Act?",
      "reference_answer": "The European Commission considers any subsequent development of the model downstream of its large pre-training run to be part of the same model lifecycle and therefore part of the same model, if performed by the same provider or on behalf of the same provider (see Section 2.2 of the Guidelines on the scope of the obligations for general-purpose AI models). Different considerations apply if another actor modifies the model (see the question 'If a general-purpose AI model is modified by another actor than the original provider, does the modifying entity become the provider?' and Section 3.2 of the Guidelines on the scope of the obligations for general-purpose AI models). This understanding of a model was chosen by the European Commission after careful consideration. In modern AI development, it is common to make small iterative changes to AI models after market placement. Providers also often perform more significant post-training to their model already placed on the market and the resulting altered model gets marketed under a new name, colloquially being referred to as a 'new model'. As our understanding of the technology is still evolving, currently it is hard to define a good boundary in what counts as a relevant or irrelevant modification for the purpose of delineating subsequent model versions into distinct models. Therefore, for now, the European Commission considers every model version descended from the same large pre-training run by the same provider to be the same model to provide legal certainty. As a result, not every 'release' that is branded as such by providers is necessarily a new model for the purposes of the AI Act, following the interpretation in the guidelines. Non-public technical details about the training process may be needed to determine whether a 'release' constitutes a new model. As models placed on the market before 2 August 2025 only need to be compliant with the AI Act starting from 2 August 2027 (Article 111(3) AI Act), it is possible that this transitional period still applies to a 'release' happening now.",
      "generated_answer": "I don't find this information in the provided context.\n\nThe context discusses general-purpose AI models that are planned for open-source release and provides details on how such models should be managed after their release. However, it does not explicitly state whether every large release of an AI model is considered a new model under the AI Act.",
      "elapsed_time_sec": 41.56212663650513
    },
    {
      "question": "What entities or roles does the AI Act apply to when it comes to models?",
      "reference_answer": "Different to the provisions on AI systems, which cover multiple different roles such as the provider, deployer, importer, or distributor, the provisions on general-purpose AI models only regulate the 'provider'. A 'provider' in this context is whoever develops a general-purpose AI model or that has a general-purpose AI model developed, and places it on the market under its own name or trademark, whether for payment or free of charge (Article 3(3) AI Act). Importantly, 'places it on the market' refers to the first commercial supply of the model for distribution or use on the Union market (Article 3(9)(10) AI Act). For example, an employer that provides its employees with licenses for a model already available on the Union market would neither be considered the provider of that model nor be required to verify its compliance under the AI Act. However, if a downstream actor integrates a model that was already made available on the Union market into its AI system, it may face obligations under the AI Act's provisions on AI systems.",
      "generated_answer": "The AI Act applies specifically to providers of general-purpose AI models. These providers have a particular role and responsibility along the AI value chain because the models they provide may form the basis for downstream systems. The act requires these providers to disclose certain information about their models, including:\n\n(a) the tasks that the model is intended to perform and the type and nature of AI systems into which it can be integrated;\n(b) the acceptable use policies applicable;\n(c) the date of release and methods of distribution;\n(d) how the model interacts, or can be used to interact, with hardware or software that is not part of the model itself, where applicable;\n(e) the versions of relevant software related to the use of the general-purpose AI model, where applicable.\n\nThese requirements are aimed at ensuring effective implementation of the regulation and enabling market surveillance authorities to investigate possible infringements.",
      "elapsed_time_sec": 63.11467480659485
    },
    {
      "question": "If a general-purpose AI model is modified by a different actor than the original provider, does the modifying entity become the provider?",
      "reference_answer": "The European Commission considers a downstream modifier to become the provider of the modified general-purpose AI model only if the modification leads to a significant change in the model's generality, capabilities, or systemic risk. An indicative criterion for this is that the training compute used for the modification is greater than a third of the training compute of the original model (see the Annex of the Guidelines on the scope of the obligations for general-purpose AI models for how 'training compute' should be understood). However, if the downstream modifier cannot be expected to know this value and cannot estimate it then the threshold should be replaced as follows. If the original model is a general-purpose AI model with systemic risk, the threshold should be replaced with a third of the threshold for a model being presumed to have high-impact capabilities (i.e., currently 10^25 floating point operations). Otherwise, it should be replaced with a third of the threshold for a model being presumed to be a general-purpose AI model (i.e., currently 10^23 floating point operations, see Section 2.1 of the Guidelines on the scope of the obligations for general-purpose AI models). This threshold is based on the expectation that a model that is modified with this amount of compute will display a significant change which warrants the downstream modifier being subject to the obligations for providers or general-purpose AI models and potentially of general-purpose AI models with systemic risk. While currently few modifications may meet this criterion, the number of downstream modifiers that become providers of general-purpose AI models may increase over time as the compute used to modify models increases. The criterion is thus primarily forward-looking, and in line with the risk-based approach of the AI Act. Therefore, the European Commission's approach may change in the future as technology and the market evolve. For further information see Section 3.2 of the Guidelines on the scope of the obligations for general-purpose AI models.",
      "generated_answer": "I don't find this information in the provided context.\n\nThe context discusses several aspects related to providers of general-purpose AI models and their responsibilities. Specifically, it mentions that if an initial provider no longer considers themselves responsible for a particular AI system under certain conditions, they should not be considered the provider for the purposes of the regulation. However, it does not address what happens when a general-purpose AI model is modified by a different actor than the original provider.\n\nTherefore, based solely on the provided context, I cannot determine whether the modifying entity becomes the provider in such cases.",
      "elapsed_time_sec": 44.248512506484985
    },
    {
      "question": "Can entities contractually determine who the provider is?",
      "reference_answer": "The AI Act recognises that the entity that trains the model may not be the one that is contractually, economically, and/or technically in charge of the development (see the provider definition stating 'has developed' in Article 3(3) AI Act). The question of under whose authority a model is trained requires a case-by-case assessment, which can take account of contractual relations amongst other factors. Some examples are provided in Section 3.1.1 of the Guidelines on the scope of the obligations for general-purpose AI models.",
      "generated_answer": "I don't find this information in the provided context.",
      "elapsed_time_sec": 20.9706552028656
    },
    {
      "question": "What legal obligations do providers of general-purpose AI models face under the AI Act as of 2 August 2025?",
      "reference_answer": "Providers of general-purpose AI models must (Article 53 AI Act): draw up and maintain technical documentation about the model, including information about the development process, to provide to the AI Office upon request; national competent authorities can also ask the AI Office to request information on their behalf when this information is necessary for them to exercise their supervisory tasks; provide information and documentation to downstream AI system providers to help them understand the model's capabilities and limitations and comply with their own obligations; implement a policy to comply with Union copyright law and related rights, including identifying and respecting rights reservations through state-of-the-art technologies; publish a sufficiently detailed summary of the content used for training the model; if they are established outside the EU, appoint an authorised representative in the Union before placing their model on the market. Providers can demonstrate compliance through the General-Purpose AI Code of Practice which was assessed as adequate, or via alternative adequate means.",
      "generated_answer": "Providers of general-purpose AI models that have been placed on the market before 2 August 2025 are required to take necessary steps in order to comply with the obligations laid down in this Regulation by 2 August 2027. This is specified in Article 3(2) of the provided context.\n\nReference: \"Providers of general-purpose AI models that have been placed on the market before 2 August 2025 shall take the necessary steps in order to comply with the obligations laid down in this Regulation by 2 August 2027.\"",
      "elapsed_time_sec": 51.17765021324158
    },
    {
      "question": "What are the additional requirements for general-purpose AI models with systemic risk?",
      "reference_answer": "In addition to the standard obligations for providers of all general-purpose AI models, providers of general-purpose AI models with systemic risk must (Article 55 AI Act): perform model evaluation using standardised protocols and state-of-the-art tools, including conducting and documenting adversarial testing to identify and mitigate systemic risks; assess and mitigate possible systemic risks at Union level, including their sources, that may stem from the development, placing on market, or use of these models; track, document, and report relevant information about serious incidents and possible corrective measures to the AI Office and, as appropriate, national authorities without undue delay; ensure adequate cybersecurity protection for both the model and its physical infrastructure, to prevent unauthorised access, theft, or leakage. Providers of such models can demonstrate compliance through adhering to the General-Purpose AI Code of Practice or show alternative adequate means of compliance. If providers choose to comply via alternative means, they must present arguments for why such means are adequate, for assessment by the European Commission.",
      "generated_answer": "The additional requirements for general-purpose AI models with systemic risk include:\n\n1. **Model Evaluations**: Providers must perform necessary model evaluations, particularly before placing the model on the market. This involves conducting and documenting adversarial testing of models, which can be done through either internal or independent external testing.\n\n2. **Continuous Assessment and Mitigation of Systemic Risks**: Providers should continuously assess and mitigate systemic risks associated with these models. This includes implementing risk-management policies to address identified risks effectively.\n\n3. **Reporting Serious Incidents**: Providers are required to keep track of, document, and report relevant information about serious incidents related to the use of general-purpose AI models with systemic risk. They must do so without undue delay to both the AI Office and, as appropriate, national competent authorities. Additionally, they should report any possible corrective measures aimed at addressing these issues.\n\n4. **Cybersecurity Protection**: Providers need to ensure an adequate level of cybersecurity protection for both the general-purpose AI model with systemic risk and its physical infrastructure.\n\nThese requirements are outlined in points (c) through (d) mentioned in your context, emphasizing the importance of continuous monitoring, documentation, reporting, and safeguarding against potential risks.",
      "elapsed_time_sec": 76.26208543777466
    },
    {
      "question": "How does the AI Act apply to general-purpose AI models released as open-source?",
      "reference_answer": "Providers of general-purpose AI models released as open-source may be exempt from certain obligations (Articles 53(2), 54(6) AI Act), specifically: the requirement to maintain technical documentation for authorities; the requirement to provide documentation to downstream AI system providers; the requirement to appoint an authorised representative (for non-EU providers). These exemptions require that the general-purpose AI model: is released under a free and open-source license, allowing access, use, modification, and distribution without monetisation; has its parameters, including weights, architecture, and usage information publicly available; is not a general-purpose AI model with systemic risk — providers of general-purpose AI models with systemic risk must comply with all the obligations for providers of general-purpose AI models, regardless of whether the model is released as open-source. These exemptions recognise that open-source models contribute to research and innovation while already providing transparency through their open nature. Nevertheless, providers whose models meet the above open-source requirements are not exempt from the copyright policy obligation or the requirement to publish a training data summary (Article 53(1)(c)(d) AI Act), since their open-source nature does not necessarily make available information on the data used for training or modifying the model, nor on how compliance with copyright law was ensured. Section 4 of the European Commission Guidelines on the scope of the obligations for general-purpose AI models provides further guidance on the open-source exemption.",
      "generated_answer": "The AI Act applies to general-purpose AI models released as open-source by ensuring compliance with obligations under this Regulation. Specifically:\n\n1. **Compliance Obligations**: After the open-source model release, necessary measures to ensure compliance with the obligations under the Regulation may become more challenging due to increased accessibility and distribution (Article 113).\n\n2. **Transparency and Openness**: General-purpose AI models released under free and open-source licenses should be considered if they make their parameters, including weights, model architecture information, and usage information publicly available. This ensures high levels of transparency and openness (Articles 103 and 104).\n\n3. **Voluntary Model Terms**: The AI Office may develop and recommend voluntary model terms for contracts between providers of high-risk AI systems and third parties that supply tools, services, components or processes used in such systems. These terms should consider possible contractual contributions to research and innovation (Article 103).\n\n4. **Monetization Consideration**: Making AI components available through open repositories should not be considered a form of monetization unless it benefits from exceptions provided for free and open-source AI components, which is generally not the case except for transactions between microenterprises (Article 104).\n\nThese points highlight that while general-purpose AI models released as open-source can contribute significantly to research and innovation, they must still adhere to the transparency and compliance requirements set forth by the AI Act.",
      "elapsed_time_sec": 86.3027274608612
    },
    {
      "question": "What is the General-Purpose AI Code of Practice?",
      "reference_answer": "The General-Purpose AI Code of Practice is a voluntary tool, prepared by independent experts in a multi-stakeholder process, designed to help industry comply with the AI Act's obligations for providers of general-purpose AI models, ensuring that general-purpose AI models placed on the European market are safe and transparent, including the most powerful ones. It outlines a way for providers of general-purpose AI models and of general-purpose AI models with systemic risk to demonstrate compliance with the AI Act's obligations laid down in Articles 53 and 55 AI Act. The Code has three chapters: Transparency and Copyright, both addressing all providers of general-purpose AI models, and Safety and Security, relevant only to the limited number of providers of the most advanced models that are subject to the AI Act's obligations for providers of general-purpose AI models with systemic risk. Any provider (or potential future provider) of a general-purpose AI model can sign the code by completing the Signatory Form and sending it to EU-AIOFFICE-CODE-SIGNATURES@ec.europa.eu. The form should be signed by a person with sufficient authority to bind the provider to the Code (e.g. a senior executive). Further information about the signing process can be found in these Q&A. For further information on which AI models are 'general-purpose' and under which conditions they are classified as 'systemic risk', see the other questions and Section 2 of the Guidelines on the scope of the obligations for general-purpose AI models. To learn more, read about the General-Purpose AI Code of Practice in these Q&A.",
      "generated_answer": "The General-Purpose AI Code of Practice refers to a set of obligations for providers of general-purpose AI models that are intended to ensure these models adhere to certain ethical and operational standards. According to Article 53, adherence may be limited unless the provider explicitly expresses interest in joining the full code.\n\nSpecifically:\n- The AI Office may invite all providers of general-purpose AI models to adhere to codes of practice.\n- For those not presenting systemic risks, adherence can be limited to obligations provided for in Article 53.\n- Providers who declare their interest in joining the full code are expected to comply with broader requirements beyond what is specified in Article 53.\n\nThe codes of practice aim to cover various aspects including:\n- Obligations for providers of general-purpose AI models, particularly focusing on content generation such as text, audio, images, or video.\n- Ensuring that these systems can accommodate a wide range of distinctive tasks.\n- Considering the system's capability to serve multiple purposes when integrated into other AI systems.\n\nThe AI Office is responsible for encouraging and facilitating the review and adaptation of these codes, ensuring they reflect the state of the art and consider diverse perspectives. Collaboration with national competent authorities, civil society organizations, experts, and relevant stakeholders is encouraged in this process.\n\nFor more detailed information on how to generate content using a general-purpose AI model or specific obligations, you can refer to points (100) and (116) in the provided context.",
      "elapsed_time_sec": 85.97538542747498
    },
    {
      "question": "How can the General-Purpose AI Code of Practice be updated?",
      "reference_answer": "The European Commission will regularly monitor and evaluate the achievement of the objectives of the General-Purpose AI Code of Practice (Code). The updating mechanism is further elaborated in the European Commission's adequacy assessment of the Code. In particular, the European Commission will consider facilitating formal updates to the Code at least every two years, for instance based on the emergence of standards, relevant technological developments, or changes in the risk landscape. To monitor the achievement of the objectives of the Code, the European Commission will remain in exchange with the signatories to understand where implementation support is necessary and may cooperate with national competent authorities, downstream providers, rightsholders, and other actors. Further, the Code leaves the European Commission's responsibility unaffected to issue guidance on the application of the AI Act, which may be of relevance for concepts in the Code. In particular, in the case of an imminent threat of large-scale irreversible harm or to address its negative effects, the European Commission will consider whether rapid guidance on the AI Act's application, or a rapid update to the Code agreed upon by the signatories, are appropriate, in addition to adequate enforcement actions. The Guidelines on the scope of the obligations for general-purpose AI models will be regularly reviewed and updated as appropriate.",
      "generated_answer": "The General-Purpose AI Code of Practice can be updated by encouraging and facilitating its review and adaptation, as mentioned in paragraph 8. The AI Office should ensure that the codes reflect the state of the art and take into account a diverse set of perspectives. Collaboration with relevant national competent authorities is necessary, and where appropriate, consultations could include civil society organizations and other relevant stakeholders and experts, including the Scientific Panel. This process ensures that the codes cover obligations for providers of general-purpose AI models, particularly in light of emerging standards.\n\nReference: Paragraph 8",
      "elapsed_time_sec": 41.86111807823181
    },
    {
      "question": "What should a provider who does not know the energy consumption of its general-purpose AI model document?",
      "reference_answer": "Pursuant to Article 53(1)(a) and Annex XI, Section 1, point 2(e), of the AI Act, providers of general-purpose AI models must document the known or estimated energy consumption of their model. If it is unknown, this estimation may be based on the computational resources used. Moreover, the European Commission is empowered to adopt a delegated act to detail measurement and calculation methodologies that providers should use to measure or estimate the energy consumption of their models, to allow for comparable and verifiable documentation. For the period until such a delegated act is adopted, the Model Documentation Form in conjunction with the Transparency Chapter of the General-Purpose AI Code of Practice (Code) provides guidance on how providers may demonstrate compliance with this requirement, under 'Energy consumption (during training and inference)'. In particular, if a Signatory to the Code does not know the energy consumption of training their model, they commit to reporting an estimated amount unless they are lacking critical information about compute or hardware which prevents them from being able to make an estimate. In this case, the Signatory commits to documenting the information they lack. For the purpose of estimating energy consumption, the AI Office will use the information available about the computational resources used for training to derive an estimate. To do so, the AI Office will rely on knowledge of what critical information the provider is lacking, available scientific resources, as well as on preliminary results from the ongoing European Commission study on energy-efficient and low-emission AI and draw on expertise from the scientific panel.",
      "generated_answer": "A provider who does not know the energy consumption of its general-purpose AI model should document information about the computational resources used to train the model, including:\n\n- The number of floating point operations.\n- Training time.\n- Other relevant details related to the training process.\n\nAccording to the context provided under point (e), if the energy consumption is unknown, it may be estimated based on this information about computational resources.",
      "elapsed_time_sec": 38.08613085746765
    },
    {
      "question": "What are the obligations of the provider if a serious incident involving its model occurs?",
      "reference_answer": "Providers of general-purpose AI models with systemic risk need to keep track of, document, and report, without undue delay, to the AI Office and, as appropriate, to national competent authorities, relevant information about serious incidents and possible corrective measures to address them (Article 55(1)(c) AI Act). The European Commission considers that this obligation covers serious cybersecurity breaches related to the model or its physical infrastructure, including the (self-)exfiltration of model parameters and cyberattacks, due to their possible implications for the obligations provided for in Article 55(1)(b) and (d) AI Act. Apart from this, the European Commission considers a 'serious incident' in the context of Chapter V AI Act as any incident or malfunctioning of a general-purpose AI model that directly or indirectly leads to any of the events listed in the corresponding definition for AI systems in Article 3(49)(a) to (d) AI Act. The Safety and Security Chapter of the General-Purpose AI Code of Practice, by way of its Commitment 9, provides a means to demonstrate compliance with this obligation.",
      "generated_answer": "If a serious incident involving its model occurs, the provider has several obligations as outlined:\n\n1. **Liaison with Authorities**: The provider or deployer must establish or suspect a causal relationship between the high-risk AI system and the serious incident.\n2. **Reporting Requirement**: Reporting is mandatory within 10 days of becoming aware of the serious incident. This ensures timely communication to relevant authorities.\n\nAdditionally, if an initial report is submitted that is incomplete:\n3. **Follow-Up Report**: The provider or deployer must submit a complete report as soon as necessary information becomes available.\n\nThese obligations are designed to ensure transparency and accountability in managing incidents related to high-risk AI systems.",
      "elapsed_time_sec": 50.379244804382324
    },
    {
      "question": "What does the European Commission expect from providers in the first year, before enforcement begins in August 2026?",
      "reference_answer": "The European Commission expects providers to comply with the obligations for providers of general-purpose AI models in the AI Act, which entered into application in August 2025. Some of these are straightforward — for instance, the notification to the AI Office when a provider is training a general-purpose AI model with systemic risk in Article 52 AI Act. Providers are expected to send these notifications without any further delay. Other obligations are more complex. For those, providers may demonstrate compliance through the General-Purpose AI Code of Practice (Code) or by demonstrating alternative adequate means of compliance. The European Commission understands that, especially regarding the more complex obligations, it may not be possible for some providers to fully implement the measures in the Code, or similarly adequate means of demonstrating compliance, by August 2025. In cases where a provider is not fully compliant with all the Code's commitments right away, the European Commission will consider them to act in good faith and will be ready to collaborate to find ways to ensure full compliance. In particular, the European Commission encourages close informal cooperation with providers during the training of their general-purpose AI models with systemic risk to facilitate compliance and ensure timely market placement. Furthermore, the European Commission expects proactive reporting by providers of general-purpose AI models with systemic risk, whether as part of commitments under the Code or as part of alternative means of demonstrating compliance. This is particularly important during the first year, where the European Commission will focus on technical exchanges on working level to facilitate providers' full compliance as soon as possible.",
      "generated_answer": "Based on the provided context, the European Commission expects providers to take appropriate measures to comply with the obligations set out in Articles 53 and 54. This expectation is outlined in Article 93(1) of the regulation, which states that \"Where necessary and appropriate, the Commission may request providers to: (a) take appropriate measures to comply with the obligations set out in Articles 53 and 54.\"\n\nThis requirement applies before enforcement begins on August 2, 2026. Therefore, providers are expected to start engaging early with the AI Office to ensure compliance with these articles.\n\nReference: Article 93(1)(a) of Regulation (EU) 2022/2065",
      "elapsed_time_sec": 57.637754678726196
    },
    {
      "question": "What are AI regulatory sandboxes, and how can providers/deployers participate?",
      "reference_answer": "AI regulatory sandboxes are safe and controlled environments for the experimentation, development, training and testing of innovative AI systems. Competent authorities supervising the sandboxes can provide guidance and advice on the interpretation of provisions of the AI Act and other relevant Union or national legislation. Any provider with an AI system that falls within the scope of the AI Act can apply for participation in an AI regulatory sandbox. Member States should have their AI regulatory sandboxes in place by 2 August 2026, after which (prospective) providers can apply for participation in the sandboxes. At least one AI regulatory sandbox will be available in each Member State.",
      "generated_answer": "AI regulatory sandboxes are controlled environments designed to allow stakeholders, particularly prospective providers or applying providers of AI systems, to interact with competent authorities. These sandboxes facilitate the raising of inquiries, seeking non-binding guidance on the conformity of innovative products, services, and business models embedding AI technologies, in accordance with Article 62(1), point (c). They are intended to support innovation by providing a framework where new AI technologies can be tested under regulated conditions.\n\nProviders/deployers can participate in AI regulatory sandboxes as follows:\n\n- **Eligibility and Selection Criteria:** Providers or prospective providers who meet the eligibility and selection criteria, which must be transparent and fair, can apply. National competent authorities are responsible for informing applicants of their decisions within three months after receiving an application.\n  \n- **Broad and Equal Access:** The AI regulatory sandboxes aim to provide broad and equal access to all interested parties, ensuring that demand is met as far as possible.\n\n- **Partnerships:** Prospective providers may also submit applications in partnerships with deployers or other relevant third parties. This allows for collaborative testing of AI technologies.\n  \n- **Pre-deployment Services:** Prospective providers, especially SMEs and start-ups, can be directed to pre-deployment services such as guidance on the implementation of this Regulation and other value-adding services.\n\n- **Free Access for SMEs/Start-ups:** The participation in these sandboxes is free of charge for SMEs including start-ups, without prejudice to any other conditions that might apply.\n\nThese arrangements are designed to foster AI innovation by ensuring flexibility for national competent authorities while supporting the development and testing of new AI technologies under controlled conditions.",
      "elapsed_time_sec": 98.6392970085144
    },
    {
      "question": "When do Member States need to establish an AI regulatory sandbox?",
      "reference_answer": "Member States should have in place an AI regulatory sandbox with national coverage by 2 August 2026.",
      "generated_answer": "Member States shall ensure that their competent authorities establish at least one AI regulatory sandbox at national level, which shall be operational by 2 August 2026. \n\nReference: \"1. Member States shall ensure that their competent authorities establish at least one AI regulatory sandbox at national level, which shall be operational by 2 August 2026.\"",
      "elapsed_time_sec": 36.55517768859863
    },
    {
      "question": "What type of AI systems can be tested with an AI regulatory sandbox?",
      "reference_answer": "Any provider with an AI system that falls within the scope of the AI Act can apply for participation in an AI regulatory sandbox. This means that AI regulatory sandboxes can (among others) supervise specific AI projects to fulfil the requirements and obligations in the AI Act, including for conformity assessment of high-risk AI systems, compliance of interactive and generative AI systems with the transparency requirements in Article 50 of the AI Act, or to enable identification, testing and implementation of effective preventive and mitigating measures to ensure specific borderline cases of AI systems do not constitute prohibited practices.",
      "generated_answer": "High-risk AI systems listed in an annex to this Regulation can be tested with an AI regulatory sandbox. According to the context, \"it is important that providers or prospective providers of such systems may also benefit from a specific regime for testing those systems in real world conditions, without participating in an AI regulatory sandbox at national level.\"",
      "elapsed_time_sec": 32.4338116645813
    },
    {
      "question": "How will the AI regulatory sandboxes and the AI Act Service Desk interact with each other?",
      "reference_answer": "The AI regulatory sandboxes and the AI Act Service Desk are expected to complement each other's aim. The AI regulatory sandboxes are designed for more specific and complex regulatory questions and challenges, and the aim of the AI Act Service Desk is to provide answers to more general questions concerning the AI Act. The regulatory challenges dealt with, and questions answered in the AI regulatory sandboxes can also support the aim of the AI Act Service Desk by making use of the lessons learnt, the guidance and answers provided in the AI regulatory sandboxes.",
      "generated_answer": "I don't find this information in the provided context.\n\nThe given context focuses on the establishment, development, implementation, operation, and supervision of AI regulatory sandboxes by the Commission. It specifies that implementing acts will be adopted to ensure common principles are followed across the Union but does not mention how AI regulatory sandboxes interact with the AI Act Service Desk.\n\nTo answer this question accurately would require additional information about the relationship between these two entities which is not provided in the given context.",
      "elapsed_time_sec": 40.37658333778381
    }
  ]
}