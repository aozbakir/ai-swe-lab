{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c26cdbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec86401",
   "metadata": {},
   "source": [
    "In this lab, we are going to build an interactive chatbot using a modular, object-oriented architecture designed around LLM (Large Language Model) utilities. The goal is not just to create a working chatbot, but to demonstrate how object-oriented design patterns, SOLID principles, and software architecture best practices can be applied in a real-world AI system. We‚Äôll start by examining the UML class diagram, which outlines the key components of our system‚Äîsuch as the LLMFactory, the different LLMBuilder implementations, and the ConversationManager‚Äîand how they interact. Throughout the session, you‚Äôll see how patterns like Factory, Builder, and Strategy work together to make the chatbot extensible and maintainable, while principles like Single Responsibility and Dependency Inversion ensure clean separation of concerns. By the end of the lab, you‚Äôll understand not only how to instantiate and configure different language models (like OpenAI, LM Studio, or HuggingFace) through a unified interface, but also how thoughtful design choices lead to flexible and scalable software systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801ec725",
   "metadata": {},
   "source": [
    "## Lab Objectives\n",
    "- Apply key software design patterns (Factory, Builder, Strategy) in the context of AI applications\n",
    "- Recognize how design principles (SOLID, DRY) improve code maintainability and flexibility\n",
    "- Analyze the flow of data and control in a component-based architecture\n",
    "- Apply event-driven design concepts to manage conversation state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543fc354",
   "metadata": {},
   "source": [
    "## Architectural notes:\n",
    "The architecture is layered because the system is organized into distinct levels of responsibility ‚Äî from model creation (via factories and builders) to orchestration (conversation management) to execution (model invocation) and finally to the user interface. Each layer depends only on the one below it, promoting clear separation of concerns and easier maintainability.\n",
    "\n",
    "It is partially event-driven because the conversation flow ‚Äî handled internally through LangGraph ‚Äî reacts to state changes and transitions between nodes based on events, rather than following a strictly linear, procedural sequence. This event-driven behavior is localized within the conversation management layer, while the overall system structure remains layered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85898821",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f283025b",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "classDiagram\n",
    "    %% Core Factory Implementation\n",
    "    class LLMFactory {\n",
    "        -Dict[str, LLMBuilder] _builders\n",
    "        +register_builder(name: str, builder: LLMBuilder) void\n",
    "        +get_builder(name: str) LLMBuilder\n",
    "        +create_from_yaml_file(config_path: Path) BaseChatModel\n",
    "        -_load_config(config_path: Path) Dict[str, Any]\n",
    "    }\n",
    "\n",
    "    %% Builder Pattern Implementation\n",
    "    class LLMBuilder {\n",
    "        <<abstract>>\n",
    "        +build(**kwargs) BaseChatModel\n",
    "    }\n",
    "\n",
    "    class OpenAILLMBuilder {\n",
    "        +build(model_name: str, api_key: str, **kwargs) ChatOpenAI\n",
    "        -_get_api_key_from_env(force_reload: bool) str\n",
    "    }\n",
    "\n",
    "    class LMStudioBuilder {\n",
    "        +build(model_name: str, base_url: str, **kwargs) ChatOpenAI\n",
    "    }\n",
    "\n",
    "    class HuggingFaceBuilder {\n",
    "        +build(model_name: str, **kwargs) HuggingFacePipeline\n",
    "    }\n",
    "\n",
    "    %% Conversation Management\n",
    "    class ConversationManager {\n",
    "        -BaseChatModel llm\n",
    "        -bool verbose\n",
    "        -str thread_id\n",
    "        -Dict config\n",
    "        -StateGraph graph\n",
    "        -Dict config_dict\n",
    "        +__init__(llm: BaseChatModel, config_path: Path, verbose: bool, thread_id: str)\n",
    "        +chat(message: str) str\n",
    "        +get_history() List[Dict[str, str]]\n",
    "        +reset() void\n",
    "        -_load_config(config_path: Path) void\n",
    "        -_build_graph() StateGraph\n",
    "        +from_yaml(llm: BaseChatModel, config_path: Path, verbose: bool, thread_id: str) ConversationManager\n",
    "    }\n",
    "\n",
    "    %% External LangChain Classes\n",
    "    class BaseChatModel {\n",
    "        <<interface>>\n",
    "        +invoke(messages: List) AIMessage\n",
    "    }\n",
    "\n",
    "    class StateGraph {\n",
    "        +add_node(name: str, func) void\n",
    "        +add_edge(source, target) void\n",
    "        +compile(checkpointer) Graph\n",
    "        +invoke(inputs: Dict, config: Dict) Dict\n",
    "    }\n",
    "\n",
    "    %% Language Model Implementations\n",
    "    class ChatOpenAI {\n",
    "        +invoke(messages: List) AIMessage\n",
    "    }\n",
    "\n",
    "    class HuggingFacePipeline {\n",
    "        +invoke(messages: List) AIMessage\n",
    "    }\n",
    "\n",
    "    %% Relationships\n",
    "    LLMBuilder <|-- OpenAILLMBuilder\n",
    "    LLMBuilder <|-- LMStudioBuilder\n",
    "    LLMBuilder <|-- HuggingFaceBuilder\n",
    "    \n",
    "    LLMFactory --> LLMBuilder : uses >\n",
    "    LLMFactory ..> BaseChatModel : creates >\n",
    "    \n",
    "    OpenAILLMBuilder ..> ChatOpenAI : creates >\n",
    "    LMStudioBuilder ..> ChatOpenAI : creates >\n",
    "    HuggingFaceBuilder ..> HuggingFacePipeline : creates >\n",
    "    \n",
    "    ChatOpenAI --|> BaseChatModel : implements\n",
    "    HuggingFacePipeline --|> BaseChatModel : implements\n",
    "    \n",
    "    ConversationManager o-- BaseChatModel : contains >\n",
    "    ConversationManager --> StateGraph : creates >\n",
    "    \n",
    "    %% Interactive Chat\n",
    "    class InteractiveChat {\n",
    "        +main() void\n",
    "    }\n",
    "    \n",
    "    InteractiveChat --> LLMFactory : uses >\n",
    "    InteractiveChat --> ConversationManager : uses >\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd35b59",
   "metadata": {},
   "source": [
    "To better understand how the components interact in this architecture, let's follow the flow of what happens when a user wants to chat with an LMStudio-based local LLM:\n",
    "\n",
    "1. **Initialization Phase**:\n",
    "   - The application starts by loading configuration from YAML files:\n",
    "     - `config.yaml` contains model configurations for different LLM providers\n",
    "     - `conversation.yaml` defines memory and conversation parameters\n",
    "   \n",
    "   - `LLMFactory` is instantiated to create the appropriate LLM implementation\n",
    "   - The factory identifies \"lmstudio\" as the requested model type\n",
    "   - It retrieves the `LMStudioBuilder` from its registry of builders\n",
    "\n",
    "2. **Model Building Phase**:\n",
    "   - `LMStudioBuilder` is activated with configuration parameters:\n",
    "     - `model_name: \"qwen2.5-7b-instruct-1m\"`\n",
    "     - `base_url: \"http://127.0.0.1:1234/v1\"`\n",
    "     - `temperature: 0.0`\n",
    "   \n",
    "   - The builder validates the connection to the local LM Studio server\n",
    "   - It constructs a `ChatOpenAI` instance configured to communicate with the local server\n",
    "   - The builder returns this instance to the factory, which returns it to the client\n",
    "\n",
    "3. **Conversation Setup Phase**:\n",
    "   - `ConversationManager` is created with the LLM instance\n",
    "   - It loads conversation configuration (memory settings, etc.)\n",
    "   - It builds a LangGraph state graph for managing conversation flow\n",
    "   - Each node in the graph represents a processing stage for messages\n",
    "\n",
    "4. **Interaction Phase**:\n",
    "   - User sends a message via `chat(message)`\n",
    "   - The message is wrapped as a `HumanMessage` object\n",
    "   - The message enters the state graph as part of the conversation state\n",
    "   - The graph executes the `call_model` node, which:\n",
    "     - Applies memory management (trimming based on configured strategy)\n",
    "     - Invokes the LLM with the current message context\n",
    "     - Captures the response and adds it to the conversation state\n",
    "   - The response is extracted and returned to the user\n",
    "\n",
    "5. **Memory Management**:\n",
    "   - Throughout this process, the `ConversationManager` maintains conversation history\n",
    "   - Messages are trimmed according to the configured strategy (e.g., keeping last N messages)\n",
    "   - The conversation state is preserved for subsequent interactions\n",
    "   - Users can retrieve the full conversation history or reset it as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34587ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\conda\\envs\\im2203\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "from im2203.llm_utils.factory import LLMFactory\n",
    "from im2203.llm_utils.chat import ConversationManager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6c8983a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the LLM factory\n",
    "llm_factory = LLMFactory()\n",
    "\n",
    "# Load LM Studio config\n",
    "lms_config = Path(\"../configs/config.yaml\")\n",
    "chat_config = Path(\"../configs/conversation.yaml\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcffcd74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"default_model\": \"openai\",\n",
      "  \"models\": {\n",
      "    \"openai\": {\n",
      "      \"name\": \"openai\",\n",
      "      \"model_name\": \"gpt-3.5-turbo\",\n",
      "      \"base_url\": null,\n",
      "      \"temperature\": 0.0\n",
      "    },\n",
      "    \"lmstudio\": {\n",
      "      \"name\": \"lmstudio\",\n",
      "      \"model_name\": \"qwen2.5-7b-instruct-1m\",\n",
      "      \"base_url\": \"http://127.0.0.1:1234/v1\",\n",
      "      \"temperature\": 0.0\n",
      "    },\n",
      "    \"huggingface\": {\n",
      "      \"name\": \"huggingface\",\n",
      "      \"model_name\": \"meta-llama/Llama-2-7b-chat-hf\",\n",
      "      \"temperature\": 0.3,\n",
      "      \"quantize_4bit\": false,\n",
      "      \"max_new_tokens\": 500,\n",
      "      \"top_k\": 50,\n",
      "      \"top_p\": 0.95\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# dump the contents of lms_config:\n",
    "with open(lms_config, \"r\") as f:\n",
    "    lms_config_dict = yaml.safe_load(f)\n",
    "\n",
    "# print the config dict in json nested format\n",
    "print(json.dumps(lms_config_dict, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fdf3fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Repos2\\.env\n",
      "Loading API key from: D:\\Repos2\\.env\n",
      "Using API key: sk-pr...JXMkA\n"
     ]
    }
   ],
   "source": [
    "llm = llm_factory.create_from_yaml_file(lms_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "857431d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ConversationManager(llm=llm, config_path=chat_config, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c3cdbcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Trimmed to 1 messages\n",
      "  - HumanMessage: message=1\n",
      "üîç Trimmed to 3 messages\n",
      "  - HumanMessage: message=1\n",
      "  - AIMessage: Hello! How can I assist you today?\n",
      "  - HumanMessage: message=2\n",
      "üîç Trimmed to 3 messages\n",
      "  - HumanMessage: message=2\n",
      "  - AIMessage: I'm here to help! What do you need assistance with?\n",
      "  - HumanMessage: message=3\n",
      "üîç Trimmed to 3 messages\n",
      "  - HumanMessage: message=3\n",
      "  - AIMessage: How can I assist you today?\n",
      "  - HumanMessage: message=4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I'm here to help! What do you need assistance with?\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.chat(\"message=1\")\n",
    "chat.chat(\"message=2\")\n",
    "chat.chat(\"message=3\")\n",
    "chat.chat(\"message=4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53196872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'message=1'},\n",
       " {'role': 'assistant', 'content': 'Hello! How can I assist you today?'},\n",
       " {'role': 'user', 'content': 'message=2'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"I'm here to help! What do you need assistance with?\"},\n",
       " {'role': 'user', 'content': 'message=3'},\n",
       " {'role': 'assistant', 'content': 'How can I assist you today?'},\n",
       " {'role': 'user', 'content': 'message=4'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"I'm here to help! What do you need assistance with?\"}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.get_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097a4762",
   "metadata": {},
   "source": [
    "Now that everything is working let's run the interactive chat from the terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de030cd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
